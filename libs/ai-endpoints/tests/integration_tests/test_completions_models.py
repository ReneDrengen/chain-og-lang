# https://platform.openai.com/docs/api-reference/completions/create
# POST https://.../v1/completions
#  model: str -- The ID of the model to use for completion.
#  prompt: str | Array[str] -- The prompt(s) to generate completions for.
#  best_of: Optional[int] (default: 1) -- An integer representing the number
#                                         of completions to generate and score.
#                                         The API will return the best completion
#                                         of the group.
#  echo: Optional[bool] (default: False) -- Whether to echo the prompt in addition
#                                           to the completion.
#  frequency_penalty: Optional[float] (default: 0.0) -- Float that penalizes new
#                                                       tokens. Range -2.0 to 2.0.
#  logit_bias: Optional[Dict[str, float]] -- Dict containing token to logit bias.
#  logprobs: Optional[int] (default: None) -- Integer representing the number of
#                                             logprobs to return. 0 means no logprobs.
#                                             Max value is 5.
#  max_tokens: Optional[int] (default: 16) -- Integer representing the maximum number
#                                             of tokens to generate.
#  n: Optional[int] (default: 1) -- Integer representing the number of completions
#                                   to generate.
#  presence_penalty: Optional[float] (default: 0.0) -- Float that penalizes new tokens
#                                                      based on whether they appear in
#                                                      the text so far. Range -2.0 to
#                                                      2.0.
#  seed: Optional[int] (default: None) -- Integer seed that attempts to make the
#                                         completions deterministic.
#  stop: Optional[str|Array[str]] -- Token at which to stop generating completions.
#                                    Up to 4 sequences.
#  stream: Optional[bool] (default: False) -- Whether to stream back partial progress.
#  stream_options: Optional[Dict["include_usage": bool]] -- Dict containing stream
#                                                           options.
#  suffix: Optional[str] -- Suffix to add to the completion.
#  temperature: Optional[float] (default: 1.0) -- Sampling temperature, between 0 and 2.
#  top_p: Optional[float] (default: 1.0) -- Alternative to temperature sampling.
#  user: Optional[str] -- User ID to associate with the request.
#
# Returns:
#  id: str -- The ID of the completion.
#  object: str -- Always "text_completion".
#  created: int -- Unix timestamp of when the completion was created.
#  model: str -- The ID of the model used to generate the completion.
#  choices: List[{"finish_reason": "stop"|"length"|"content_filter",
#                 "index": int,
#                 "text": str,
#                 "logprobs": Optional[{"text_offset": array,
#                                       "token_logprobs": array,
#                                       "tokens": array,
#                                       "top_logprobs": array}]}] --
#    List of completions generated by the model.
#  usage: {"completion_tokens": int,
#          "prompt_tokens": int,
#          "total_tokens": int} -- Usage statistics for the model.
#  system_fingerprint: str -- System fingerprint of the model used to generate
#                             the completion.


from typing import Any, Callable, Tuple

import pytest

from langchain_nvidia_ai_endpoints import NVIDIA


def invoke(llm: NVIDIA, prompt: str, **kwargs: Any) -> Tuple[str, int]:
    return llm.invoke(prompt, **kwargs), 1


def stream(llm: NVIDIA, prompt: str, **kwargs: Any) -> Tuple[str, int]:
    response = ""
    count = 0
    for chunk in llm.stream(prompt, **kwargs):
        response += chunk
        count += 1
    return response, count


@pytest.mark.parametrize(
    "func, count", [(invoke, 0), (stream, 1)], ids=["invoke", "stream"]
)
def test_basic(completions_model: str, mode: dict, func: Callable, count: int) -> None:
    llm = NVIDIA(model=completions_model, **mode)
    response, cnt = func(llm, "Hello, my name is")
    assert isinstance(response, str)
    assert cnt > count, "Should have received more chunks"


@pytest.mark.parametrize(
    "param, value",
    [
        ("frequency_penalty", 0.5),
        ("max_tokens", 32),
        ("presence_penalty", 0.5),
        ("seed", 1234),
        ("stop", "Hello"),
        ("temperature", 0.5),
        ("top_p", 0.5),
    ],
)
@pytest.mark.parametrize("func", [invoke, stream], ids=["invoke", "stream"])
def test_params(
    completions_model: str, mode: dict, param: str, value: Any, func: Callable
) -> None:
    llm = NVIDIA(model=completions_model, **mode)
    response, _ = func(llm, "Hello, my name is", **{param: value})
    assert isinstance(response, str)


@pytest.mark.parametrize(
    "param, value",
    [
        ("best_of", 5),
        ("echo", True),
        ("logit_bias", {"hello": 1.0}),
        ("logprobs", 2),
        ("n", 2),
        ("suffix", "Hello"),
        ("user", "1234"),
    ],
)
@pytest.mark.parametrize("func", [invoke, stream], ids=["invoke", "stream"])
@pytest.mark.xfail(reason="Not consistently implemented")
def test_params_incomplete(
    completions_model: str, mode: dict, param: str, value: Any, func: Callable
) -> None:
    llm = NVIDIA(model=completions_model, **mode)
    response, _ = func(llm, "Hello, my name is", **{param: value})
    assert isinstance(response, str)


def test_invoke_with_stream_true(completions_model: str, mode: dict) -> None:
    llm = NVIDIA(model=completions_model, **mode)
    with pytest.warns(UserWarning) as record:
        response = llm.invoke("Hello, my name is", stream=True)
    assert isinstance(response, str)
    assert len(record) == 1
    assert "stream set to true" in str(record[0].message)
    assert "ignoring" in str(record[0].message)


def test_stream_with_stream_false(completions_model: str, mode: dict) -> None:
    llm = NVIDIA(model=completions_model, **mode)
    with pytest.warns(UserWarning) as record:
        response = next(llm.stream("Hello, my name is", stream=False))
    assert isinstance(response, str)
    assert len(record) == 1
    assert "stream set to false" in str(record[0].message)
    assert "ignoring" in str(record[0].message)


# todo: check stream_options
