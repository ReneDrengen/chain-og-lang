{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9d3415-fe08-4cc0-bbb8-b582cb01e754",
   "metadata": {},
   "source": [
    "# Nvidia TensorRT-LLM\n",
    "TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build TensorRT engines that contain state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs.\n",
    "<br>\n",
    "[TensorRT-LLM Github](https://github.com/NVIDIA/TensorRT-LLM)\n",
    "\n",
    "## TensorRT-LLM Environment Setup\n",
    "Since TensorRT-LLM is a SDK for interacting with local models in process there are a few environment steps that must be followed to ensure that the TensorRT-LLM setup can be used.\n",
    "<br>\n",
    "1. Nvidia Cuda 12.2 or higher is currently required to run TensorRT-LLM\n",
    "2. Install `tensorrt_llm` via pip with `pip install tensorrt_llm==0.8.0 --extra-index-url https://pypi.nvidia.com --extra-index-url https://download.pytorch.org/whl/`\n",
    "3. For this example we will use Llama2. The Llama2 model files need to be created via scripts following the instructions [here](https://github.com/NVIDIA/TensorRT-LLM/tree/main/examples/llama)<br>\n",
    "   * The following files will be created from following the step above\n",
    "   * `rank0.engine`: The main output of the build script, containing the executable graph of operations with the model weights embedded\n",
    "   * `config.json`: Includes detailed information about the model, like its general structure and precision, as well as information about which plug-ins were incorporated into the engine\n",
    "5. `mkdir model`\n",
    "6. Move all of the files mentioned above to the model directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af02aa0-be5a-4121-9853-7af277c257c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-nvidia-trt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e836a3-be37-416a-8af6-5e53562789e4",
   "metadata": {},
   "source": [
    "## Create the TrtLlmAPI instance\n",
    "Call `invoke` with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ac6ac-b728-47d2-ae41-11a5cf4c1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_trt.llms import TrtLlmAPI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm = TrtLlmAPI(\n",
    "    model_path=\"./model\",\n",
    "    tokenizer_dir=\"meta-llama/Llama-2-7b-chat\",\n",
    ")\n",
    "chain = prompt | llm\n",
    "print(chain.invoke({\"question\": \"What is important about Half Life 2 RTX?\"}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
